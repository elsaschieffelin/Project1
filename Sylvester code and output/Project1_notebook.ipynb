{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from config import api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': {'id': 6,\n",
       "  'type': 'Feature',\n",
       "  'geometry': {'type': 'Point', 'coordinates': [-95.36327, 29.76328]},\n",
       "  'properties': {'proximity': {'ocean': True},\n",
       "   'name': 'Houston',\n",
       "   'admin': 'TX',\n",
       "   'population': 2099451,\n",
       "   'datasets': ['LOCA', 'NEX-GDDP'],\n",
       "   'region': 47}},\n",
       " 'dataset': 'LOCA',\n",
       " 'scenario': 'historical',\n",
       " 'indicator': {'name': 'extreme_precipitation_events',\n",
       "  'label': 'Extreme Precipitation Events',\n",
       "  'description': 'Total number of times per period daily average precipitation rate exceeds the specified percentile of historic observations',\n",
       "  'valid_aggregations': ['yearly',\n",
       "   'quarterly',\n",
       "   'monthly',\n",
       "   'offset_yearly',\n",
       "   'custom'],\n",
       "  'variables': ['pr'],\n",
       "  'available_units': ['count'],\n",
       "  'default_units': 'count',\n",
       "  'parameters': [{'name': 'agg',\n",
       "    'description': \"A list of comma separated aggregation types to return. Valid choices are 'min', 'max', 'avg', 'median', 'stddev', 'stdev', and 'XXth'. If using 'XXth', replace the XX with a number between 1-99 to return that percentile. For example, '99th' returns the value of the 99th percentile. The 'XXth' option can be provided multiple times with different values. 'stdev' is an alias to 'stddev'. Defaults to 'min,max,avg'.\",\n",
       "    'required': False,\n",
       "    'default': 'min,max,avg'},\n",
       "   {'name': 'custom_time_agg',\n",
       "    'description': \"Used in conjunction with the 'custom' time_aggregation value. A list of comma separated month-day pairs defining the time intervals to aggregate within. Data points will only be assigned to one aggregation, and for overlapping intervals the interval defined first will take precedence. Dates are formmatted MM-DD and pairs are formatted 'start:end'. Examples: '3-1:5-31', '1-1:6-30,7-1:12-31'\",\n",
       "    'required': False},\n",
       "   {'name': 'dataset',\n",
       "    'description': 'A single value defining which provider to use for raw climate data. If not provided, defaults to NEX-GDDP.',\n",
       "    'required': False,\n",
       "    'default': 'NEX-GDDP'},\n",
       "   {'name': 'historic_range',\n",
       "    'description': 'The 30 year range of past years used to define the historic norm. Get the available options by querying the historic ranges endpoint. Defaults to the most recent period in the historical data.',\n",
       "    'required': False,\n",
       "    'default': '1971'},\n",
       "   {'name': 'models',\n",
       "    'description': 'A list of comma separated model names to filter the indicator by. The indicator values in the response will only use the selected models. If not provided, defaults to all models.',\n",
       "    'required': False,\n",
       "    'default': 'all'},\n",
       "   {'name': 'percentile',\n",
       "    'description': 'The percentile threshold used to determine the appropriate comparative level of an event or measurement. Must be an integer in the range [0,100]. Defaults to 99',\n",
       "    'required': False,\n",
       "    'default': 99},\n",
       "   {'name': 'time_aggregation',\n",
       "    'description': \"Time granularity to group data by for result structure. Valid aggregations depend on indicator. Can be 'yearly', 'offset_yearly', 'quarterly', 'monthly', or 'custom'. Defaults to 'yearly'. If 'custom', 'custom_time_agg' parameter must be set.\",\n",
       "    'required': False,\n",
       "    'default': 'yearly'},\n",
       "   {'name': 'units',\n",
       "    'description': 'Units in which to return the data. Defaults to Imperial units (Fahrenheit for temperature indicators and inches for precipitation).',\n",
       "    'required': False,\n",
       "    'default': 'count'},\n",
       "   {'name': 'years',\n",
       "    'description': \"A list of comma separated year ranges to filter the response by. Defaults to all years available. A year range is of the form 'start[:end]'. Examples: '2010', '2010:2020', '2010:2020,2030', '2010:2020,2030:2040'\",\n",
       "    'required': False,\n",
       "    'default': 'all'}]},\n",
       " 'climate_models': ['ACCESS1-0',\n",
       "  'CanESM2',\n",
       "  'CESM1-CAM5',\n",
       "  'FGOALS-g2',\n",
       "  'GFDL-ESM2M',\n",
       "  'GISS-E2-H',\n",
       "  'HadGEM2-AO',\n",
       "  'IPSL-CM5A-MR',\n",
       "  'MIROC-ESM-CHEM',\n",
       "  'MPI-ESM-MR'],\n",
       " 'time_aggregation': 'yearly',\n",
       " 'units': 'count',\n",
       " 'data': {'1950': {'max': 13, 'min': 4, 'avg': 8.2},\n",
       "  '1951': {'max': 20, 'min': 3, 'avg': 9.7},\n",
       "  '1952': {'max': 13, 'min': 4, 'avg': 8.5},\n",
       "  '1953': {'max': 18, 'min': 2, 'avg': 9.0},\n",
       "  '1954': {'max': 16, 'min': 5, 'avg': 8.4},\n",
       "  '1955': {'max': 13, 'min': 4, 'avg': 9.0},\n",
       "  '1956': {'max': 21, 'min': 3, 'avg': 12.0},\n",
       "  '1957': {'max': 17, 'min': 2, 'avg': 8.7},\n",
       "  '1958': {'max': 18, 'min': 1, 'avg': 8.1},\n",
       "  '1959': {'max': 22, 'min': 0, 'avg': 9.2},\n",
       "  '1960': {'max': 19, 'min': 6, 'avg': 11.7},\n",
       "  '1961': {'max': 18, 'min': 5, 'avg': 9.2},\n",
       "  '1962': {'max': 16, 'min': 4, 'avg': 9.7},\n",
       "  '1963': {'max': 21, 'min': 4, 'avg': 8.1},\n",
       "  '1964': {'max': 10, 'min': 0, 'avg': 4.3},\n",
       "  '1965': {'max': 14, 'min': 7, 'avg': 9.8},\n",
       "  '1966': {'max': 11, 'min': 5, 'avg': 7.9},\n",
       "  '1967': {'max': 14, 'min': 5, 'avg': 8.4},\n",
       "  '1968': {'max': 18, 'min': 4, 'avg': 9.0},\n",
       "  '1969': {'max': 20, 'min': 1, 'avg': 9.7},\n",
       "  '1970': {'max': 22, 'min': 3, 'avg': 10.4},\n",
       "  '1971': {'max': 20, 'min': 5, 'avg': 9.7},\n",
       "  '1972': {'max': 13, 'min': 3, 'avg': 8.1},\n",
       "  '1973': {'max': 16, 'min': 0, 'avg': 8.9},\n",
       "  '1974': {'max': 12, 'min': 5, 'avg': 8.8},\n",
       "  '1975': {'max': 14, 'min': 4, 'avg': 8.9},\n",
       "  '1976': {'max': 15, 'min': 2, 'avg': 8.9},\n",
       "  '1977': {'max': 13, 'min': 4, 'avg': 7.7},\n",
       "  '1978': {'max': 9, 'min': 3, 'avg': 6.4},\n",
       "  '1979': {'max': 20, 'min': 3, 'avg': 10.1},\n",
       "  '1980': {'max': 18, 'min': 6, 'avg': 11.1},\n",
       "  '1981': {'max': 16, 'min': 4, 'avg': 9.8},\n",
       "  '1982': {'max': 17, 'min': 6, 'avg': 10.1},\n",
       "  '1983': {'max': 19, 'min': 2, 'avg': 9.0},\n",
       "  '1984': {'max': 23, 'min': 3, 'avg': 9.0},\n",
       "  '1985': {'max': 14, 'min': 4, 'avg': 8.7},\n",
       "  '1986': {'max': 15, 'min': 4, 'avg': 9.3},\n",
       "  '1987': {'max': 13, 'min': 4, 'avg': 8.4},\n",
       "  '1988': {'max': 16, 'min': 7, 'avg': 10.4},\n",
       "  '1989': {'max': 18, 'min': 2, 'avg': 9.5},\n",
       "  '1990': {'max': 17, 'min': 6, 'avg': 10.7},\n",
       "  '1991': {'max': 10, 'min': 2, 'avg': 6.9},\n",
       "  '1992': {'max': 17, 'min': 5, 'avg': 11.3},\n",
       "  '1993': {'max': 19, 'min': 5, 'avg': 11.2},\n",
       "  '1994': {'max': 14, 'min': 3, 'avg': 8.1},\n",
       "  '1995': {'max': 20, 'min': 1, 'avg': 9.8},\n",
       "  '1996': {'max': 18, 'min': 2, 'avg': 9.0},\n",
       "  '1997': {'max': 18, 'min': 3, 'avg': 10.9},\n",
       "  '1998': {'max': 23, 'min': 4, 'avg': 10.1},\n",
       "  '1999': {'max': 13, 'min': 5, 'avg': 8.7},\n",
       "  '2000': {'max': 18, 'min': 2, 'avg': 10.3},\n",
       "  '2001': {'max': 16, 'min': 4, 'avg': 10.1},\n",
       "  '2002': {'max': 14, 'min': 2, 'avg': 9.7},\n",
       "  '2003': {'max': 11, 'min': 4, 'avg': 6.8},\n",
       "  '2004': {'max': 19, 'min': 2, 'avg': 9.2},\n",
       "  '2005': {'max': 12, 'min': 3, 'avg': 7.2}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up api key parameter\n",
    "params = {\"Authorization\": \"TOKEN \" + api_key}\n",
    "\n",
    "#Create some starter tables\n",
    "\n",
    "#Extreme Heat Events\n",
    "#create a starter dataframe \n",
    "starter_heat_url = \"https://app.climate.azavea.com/api/climate-data/6/historical/indicator/extreme_heat_events/?historic_range=1951&percentile=95&models=ACCESS1-0,CanESM2,CESM1-CAM5,FGOALS-g2,GFDL-ESM2M,GISS-E2-H,HadGEM2-AO,IPSL-CM5A-MR,MIROC-ESM-CHEM,MPI-ESM-MR&time_aggregation=yearly&units=count&dataset=LOCA\"\n",
    "starter_heat_response = requests.get(starter_heat_url, headers=params)\n",
    "starter_heat_json = starter_heat_response.json()\n",
    "starter_heat_df = pd.DataFrame(starter_heat_json['data'])\n",
    "# get a df with only averages\n",
    "starter_heat_df.drop(['min', 'max'], inplace =True)\n",
    "# rename to avoid confusion later\n",
    "starter_heat_df.rename(index={'avg': 'Houston_avg'}, inplace=True)\n",
    "starter_heat_df\n",
    "\n",
    "#Extreme Precipitation Events\n",
    "starter_rain_url = \"https://app.climate.azavea.com/api/climate-data/6/historical/indicator/extreme_precipitation_events/?historic_range=1951&percentile=95&models=ACCESS1-0,CanESM2,CESM1-CAM5,FGOALS-g2,GFDL-ESM2M,GISS-E2-H,HadGEM2-AO,IPSL-CM5A-MR,MIROC-ESM-CHEM,MPI-ESM-MR&time_aggregation=yearly&units=count&dataset=LOCA\"\n",
    "starter_rain_response = requests.get(starter_rain_url, headers=params)\n",
    "starter_rain_json = starter_rain_response.json()\n",
    "starter_rain_df = pd.DataFrame(starter_rain_json['data'])\n",
    "starter_rain_df.drop(['min', 'max'], inplace =True)\n",
    "starter_rain_df.rename(index={'avg': 'Houston_avg'}, inplace=True)\n",
    "\n",
    "starter_rain_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# created a random list of 100 city codes\n",
    "# added them into a static list so I dont have to run the code on new cities every time\n",
    "city_nums = [ 263, 1533,  591, 761, 449, 15, 120,  644, 290, 507, 1313,\n",
    "        564, 30, 1487, 58,  284,  644, 1263,  923, 1038, 1209, 193,\n",
    "        1269, 1030,  954,   56, 1391, 1710,  718,  339, 1328,  854,  190,\n",
    "        252,  712,   16,  652,  219, 1649,  676, 1196, 1246,  281, 1761,\n",
    "        322,  625, 1771,  721,  901, 1254,  116, 1623, 1617, 1715, 1691,\n",
    "       1779,  902, 1040,  475,  957, 1275, 1190, 1540,  743,  198, 1742,\n",
    "       1127,  109,  759, 1456,  527,  874, 1257,  555, 1410, 1621,  888,\n",
    "        280,  608,  116, 1532, 1515,  935, 1339, 1070,  837,  846,   67,\n",
    "       1415,  289,   93,  332, 1667,  433, 1454,  635,    6,  988, 1380,\n",
    "        870]\n",
    "# create lists that will hold data frames to be concatenated later\n",
    "heat_frames = [starter_heat_df]\n",
    "rain_frames = [starter_rain_df]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to acquire heat data for city #263\n",
      "Attempting to acquire heat data for city #1533\n",
      "Attempting to acquire heat data for city #591\n",
      "Attempting to acquire heat data for city #761\n",
      "Attempting to acquire heat data for city #449\n",
      "Attempting to acquire heat data for city #15\n",
      "Attempting to acquire heat data for city #120\n",
      "Attempting to acquire heat data for city #644\n",
      "Attempting to acquire heat data for city #290\n",
      "Attempting to acquire heat data for city #507\n",
      "Attempting to acquire heat data for city #1313\n",
      "Attempting to acquire heat data for city #564\n",
      "Attempting to acquire heat data for city #30\n",
      "Attempting to acquire heat data for city #1487\n",
      "Attempting to acquire heat data for city #58\n",
      "Attempting to acquire heat data for city #284\n",
      "Attempting to acquire heat data for city #644\n",
      "Attempting to acquire heat data for city #1263\n",
      "Attempting to acquire heat data for city #923\n",
      "Attempting to acquire heat data for city #1038\n",
      "Attempting to acquire heat data for city #1209\n",
      "Attempting to acquire heat data for city #193\n",
      "Attempting to acquire heat data for city #1269\n",
      "Attempting to acquire heat data for city #1030\n",
      "Attempting to acquire heat data for city #954\n",
      "Attempting to acquire heat data for city #56\n",
      "Attempting to acquire heat data for city #1391\n",
      "Attempting to acquire heat data for city #1710\n",
      "Attempting to acquire heat data for city #718\n",
      "Attempting to acquire heat data for city #339\n",
      "Attempting to acquire heat data for city #1328\n",
      "Attempting to acquire heat data for city #854\n",
      "Attempting to acquire heat data for city #190\n",
      "Attempting to acquire heat data for city #252\n",
      "Attempting to acquire heat data for city #712\n",
      "Attempting to acquire heat data for city #16\n",
      "Attempting to acquire heat data for city #652\n",
      "Attempting to acquire heat data for city #219\n",
      "Attempting to acquire heat data for city #1649\n",
      "Attempting to acquire heat data for city #676\n",
      "Attempting to acquire heat data for city #1196\n",
      "Attempting to acquire heat data for city #1246\n",
      "Attempting to acquire heat data for city #281\n",
      "Attempting to acquire heat data for city #1761\n",
      "Attempting to acquire heat data for city #322\n",
      "Attempting to acquire heat data for city #625\n",
      "Attempting to acquire heat data for city #1771\n",
      "Attempting to acquire heat data for city #721\n",
      "Attempting to acquire heat data for city #901\n",
      "Attempting to acquire heat data for city #1254\n",
      "Attempting to acquire heat data for city #116\n",
      "Attempting to acquire heat data for city #1623\n",
      "Attempting to acquire heat data for city #1617\n",
      "Attempting to acquire heat data for city #1715\n",
      "Attempting to acquire heat data for city #1691\n",
      "Attempting to acquire heat data for city #1779\n",
      "failed to collect data, city value not found\n",
      "Attempting to acquire heat data for city #902\n",
      "Attempting to acquire heat data for city #1040\n",
      "Attempting to acquire heat data for city #475\n",
      "Attempting to acquire heat data for city #957\n",
      "Attempting to acquire heat data for city #1275\n",
      "Attempting to acquire heat data for city #1190\n",
      "Attempting to acquire heat data for city #1540\n",
      "Attempting to acquire heat data for city #743\n",
      "Attempting to acquire heat data for city #198\n",
      "Attempting to acquire heat data for city #1742\n",
      "Attempting to acquire heat data for city #1127\n",
      "Attempting to acquire heat data for city #109\n",
      "Attempting to acquire heat data for city #759\n",
      "Attempting to acquire heat data for city #1456\n",
      "Attempting to acquire heat data for city #527\n",
      "Attempting to acquire heat data for city #874\n",
      "Attempting to acquire heat data for city #1257\n",
      "Attempting to acquire heat data for city #555\n",
      "Attempting to acquire heat data for city #1410\n",
      "Attempting to acquire heat data for city #1621\n",
      "Attempting to acquire heat data for city #888\n",
      "Attempting to acquire heat data for city #280\n",
      "Attempting to acquire heat data for city #608\n",
      "Attempting to acquire heat data for city #116\n",
      "Attempting to acquire heat data for city #1532\n",
      "Attempting to acquire heat data for city #1515\n",
      "Attempting to acquire heat data for city #935\n",
      "Attempting to acquire heat data for city #1339\n",
      "Attempting to acquire heat data for city #1070\n",
      "Attempting to acquire heat data for city #837\n",
      "Attempting to acquire heat data for city #846\n",
      "Attempting to acquire heat data for city #67\n",
      "Attempting to acquire heat data for city #1415\n",
      "Attempting to acquire heat data for city #289\n",
      "Attempting to acquire heat data for city #93\n",
      "Attempting to acquire heat data for city #332\n",
      "Attempting to acquire heat data for city #1667\n",
      "Attempting to acquire heat data for city #433\n",
      "Attempting to acquire heat data for city #1454\n",
      "Attempting to acquire heat data for city #635\n",
      "Attempting to acquire heat data for city #6\n",
      "Attempting to acquire heat data for city #988\n",
      "Attempting to acquire heat data for city #1380\n",
      "Attempting to acquire heat data for city #870\n"
     ]
    }
   ],
   "source": [
    "## EXTREME HEAT EVENTS\n",
    "for num in city_nums:\n",
    "    \n",
    "    try:\n",
    "        #create url and retrieve data\n",
    "        response_heat = requests.get(\"https://app.climate.azavea.com/api/climate-data/\"+ str(num) + \"/historical/indicator/extreme_heat_events/?historic_range=1951&percentile=95&models=ACCESS1-0,CanESM2,CESM1-CAM5,FGOALS-g2,GFDL-ESM2M,GISS-E2-H,HadGEM2-AO,IPSL-CM5A-MR,MIROC-ESM-CHEM,MPI-ESM-MR&time_aggregation=yearly&units=count&dataset=LOCA\", headers=params)\n",
    "        json_heat = response_heat.json()\n",
    "        print(f\"Attempting to acquire heat data for city #{num}\")\n",
    "        #create data frame from data\n",
    "        df_heat = pd.DataFrame(json_heat['data'])\n",
    "        df_heat.drop(['min', 'max'], inplace=True)\n",
    "        #replace index names with name of city\n",
    "        heat_name = json_heat['city']['properties']['name']\n",
    "        df_heat.rename(index={'avg': heat_name + '_avg'}, inplace=True)\n",
    "        #add to list of dataframes\n",
    "        heat_frames.append(df_heat)\n",
    "        \n",
    "        \n",
    "    except KeyError:\n",
    "        print(\"failed to collect data, city value not found\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.concat(heat_frames)\n",
    "#print(heat_name)\n",
    "#df_heat\n",
    "new\n",
    "#export to csv\n",
    "new.to_csv('heat_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to acquire precipitation data for city #263\n",
      "Attempting to acquire precipitation data for city #1533\n",
      "Attempting to acquire precipitation data for city #591\n",
      "Attempting to acquire precipitation data for city #761\n",
      "Attempting to acquire precipitation data for city #449\n",
      "Attempting to acquire precipitation data for city #15\n",
      "Attempting to acquire precipitation data for city #120\n",
      "Attempting to acquire precipitation data for city #644\n",
      "Attempting to acquire precipitation data for city #290\n",
      "Attempting to acquire precipitation data for city #507\n",
      "Attempting to acquire precipitation data for city #1313\n",
      "Attempting to acquire precipitation data for city #564\n",
      "Attempting to acquire precipitation data for city #30\n",
      "Attempting to acquire precipitation data for city #1487\n",
      "Attempting to acquire precipitation data for city #58\n",
      "Attempting to acquire precipitation data for city #284\n",
      "Attempting to acquire precipitation data for city #644\n",
      "Attempting to acquire precipitation data for city #1263\n",
      "Attempting to acquire precipitation data for city #923\n",
      "Attempting to acquire precipitation data for city #1038\n",
      "Attempting to acquire precipitation data for city #1209\n",
      "Attempting to acquire precipitation data for city #193\n",
      "Attempting to acquire precipitation data for city #1269\n",
      "Attempting to acquire precipitation data for city #1030\n",
      "Attempting to acquire precipitation data for city #954\n",
      "Attempting to acquire precipitation data for city #56\n",
      "Attempting to acquire precipitation data for city #1391\n",
      "Attempting to acquire precipitation data for city #1710\n",
      "Attempting to acquire precipitation data for city #718\n",
      "Attempting to acquire precipitation data for city #339\n",
      "Attempting to acquire precipitation data for city #1328\n",
      "Attempting to acquire precipitation data for city #854\n",
      "Attempting to acquire precipitation data for city #190\n",
      "Attempting to acquire precipitation data for city #252\n",
      "Attempting to acquire precipitation data for city #712\n",
      "Attempting to acquire precipitation data for city #16\n",
      "Attempting to acquire precipitation data for city #652\n",
      "Attempting to acquire precipitation data for city #219\n",
      "Attempting to acquire precipitation data for city #1649\n",
      "Attempting to acquire precipitation data for city #676\n",
      "Attempting to acquire precipitation data for city #1196\n",
      "Attempting to acquire precipitation data for city #1246\n",
      "Attempting to acquire precipitation data for city #281\n",
      "Attempting to acquire precipitation data for city #1761\n",
      "Attempting to acquire precipitation data for city #322\n",
      "Attempting to acquire precipitation data for city #625\n",
      "Attempting to acquire precipitation data for city #1771\n",
      "Attempting to acquire precipitation data for city #721\n",
      "Attempting to acquire precipitation data for city #901\n",
      "Attempting to acquire precipitation data for city #1254\n",
      "Attempting to acquire precipitation data for city #116\n",
      "Attempting to acquire precipitation data for city #1623\n",
      "Attempting to acquire precipitation data for city #1617\n",
      "Attempting to acquire precipitation data for city #1715\n",
      "Attempting to acquire precipitation data for city #1691\n",
      "Attempting to acquire precipitation data for city #1779\n",
      "failed to collect data, city value not found\n",
      "Attempting to acquire precipitation data for city #902\n",
      "Attempting to acquire precipitation data for city #1040\n",
      "Attempting to acquire precipitation data for city #475\n",
      "Attempting to acquire precipitation data for city #957\n",
      "Attempting to acquire precipitation data for city #1275\n",
      "Attempting to acquire precipitation data for city #1190\n",
      "Attempting to acquire precipitation data for city #1540\n",
      "Attempting to acquire precipitation data for city #743\n",
      "Attempting to acquire precipitation data for city #198\n",
      "Attempting to acquire precipitation data for city #1742\n",
      "Attempting to acquire precipitation data for city #1127\n",
      "Attempting to acquire precipitation data for city #109\n",
      "Attempting to acquire precipitation data for city #759\n",
      "Attempting to acquire precipitation data for city #1456\n",
      "Attempting to acquire precipitation data for city #527\n",
      "Attempting to acquire precipitation data for city #874\n",
      "Attempting to acquire precipitation data for city #1257\n",
      "Attempting to acquire precipitation data for city #555\n",
      "Attempting to acquire precipitation data for city #1410\n",
      "Attempting to acquire precipitation data for city #1621\n",
      "Attempting to acquire precipitation data for city #888\n",
      "Attempting to acquire precipitation data for city #280\n",
      "Attempting to acquire precipitation data for city #608\n",
      "Attempting to acquire precipitation data for city #116\n",
      "Attempting to acquire precipitation data for city #1532\n",
      "Attempting to acquire precipitation data for city #1515\n",
      "Attempting to acquire precipitation data for city #935\n",
      "Attempting to acquire precipitation data for city #1339\n",
      "Attempting to acquire precipitation data for city #1070\n",
      "Attempting to acquire precipitation data for city #837\n",
      "Attempting to acquire precipitation data for city #846\n",
      "Attempting to acquire precipitation data for city #67\n",
      "Attempting to acquire precipitation data for city #1415\n",
      "Attempting to acquire precipitation data for city #289\n",
      "Attempting to acquire precipitation data for city #93\n",
      "Attempting to acquire precipitation data for city #332\n",
      "Attempting to acquire precipitation data for city #1667\n",
      "Attempting to acquire precipitation data for city #433\n",
      "Attempting to acquire precipitation data for city #1454\n",
      "Attempting to acquire precipitation data for city #635\n",
      "Attempting to acquire precipitation data for city #6\n",
      "Attempting to acquire precipitation data for city #988\n",
      "Attempting to acquire precipitation data for city #1380\n",
      "Attempting to acquire precipitation data for city #870\n"
     ]
    }
   ],
   "source": [
    "## EXTREME PRECIPITATION EVENTS\n",
    "for num in city_nums:\n",
    "    \n",
    "    try:\n",
    "        #create url and retrieve data\n",
    "        response_rain = requests.get(\"https://app.climate.azavea.com/api/climate-data/\"+ str(num)+ \"/historical/indicator/extreme_precipitation_events/?historic_range=1951&percentile=95&models=ACCESS1-0,CanESM2,CESM1-CAM5,FGOALS-g2,GFDL-ESM2M,GISS-E2-H,HadGEM2-AO,IPSL-CM5A-MR,MIROC-ESM-CHEM,MPI-ESM-MR&time_aggregation=yearly&units=count&dataset=LOCA\", headers=params)\n",
    "        json_rain = response_rain.json()\n",
    "        print(f\"Attempting to acquire precipitation data for city #{num}\")\n",
    "        #create data frame from data\n",
    "        df_rain = pd.DataFrame(json_rain['data'])\n",
    "        df_rain.drop(['min', 'max'], inplace=True)\n",
    "        #replace index names with name of city\n",
    "        rain_name = json_rain['city']['properties']['name']\n",
    "        df_rain.rename(index={'avg': rain_name + '_avg'}, inplace=True)\n",
    "        #add to list of dataframes\n",
    "        rain_frames.append(df_rain)\n",
    "        \n",
    "        \n",
    "    except KeyError:\n",
    "        print(\"failed to collect data, city value not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rain = pd.concat(rain_frames)\n",
    "new_rain\n",
    "new_rain.to_csv('rain_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
